# -*- coding: utf-8 -*-
"""book-recommendation-system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZN_DFraw9RjFwfrKPwfpE0HM2GZVF4tI
"""

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive/CAPSTONE

"""# Import Necessary Libraries"""

!pip install fuzzywuzzy
!pip install langid
!pip install tensorflow

from fuzzywuzzy import fuzz
from fuzzywuzzy import process
import pandas as pd
import json
import matplotlib.pyplot as plt
import warnings
import langid
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow.keras.models import load_model

# Ignore all warnings
warnings.filterwarnings("ignore")

"""# Data Preparation and EDA"""

df=pd.read_csv('/content/drive/MyDrive/CAPSTONE/dataset fix/Goodreads_BestBooksEver_1-10000.csv')

df.shape

df.head()

df.columns

df.isna().sum()

"""There are 100 null values in the bookGenres column so we will simply drop them because 100 values does not matter that much"""

df.dropna(inplace=True)

df.drop_duplicates(inplace=True)

df.shape

df['bookGenres'][0]

"""Now as you can see the genres for books are some random string with some numbers in it so we need to extract those genres and store them seperatly so i am using set to make the set of genres for a book"""

def extract_genres(input_string):
    genres_data = input_string.split('|')
    extracted_genres = set()
    for genre_entry in genres_data:
        genre_parts = genre_entry.split('/')
        if len(genre_parts) >= 2:
            genre_name = genre_parts[0]
            extracted_genres.add(genre_name)
    return extracted_genres

df['cleaned_bookGenres'] = df["bookGenres"].apply(extract_genres)

df['bookTitle'][0]

df['bookGenres'][0]

df['cleaned_bookGenres'][0]

df.columns

"""Now as we clean the genres and we have cleaned_bookGenres so there is no need for bookGenres so i am going to drop it"""

df.drop(['bookGenres'],inplace=True,axis=1)

df.head()

"""# A little Extra

Here i want to do a little extra thing i want the recommended books to me in same language because most of the reader prefer to read the books in the same language so i am going to add the extra column bookLang in the dataset
"""

def detect_lang(input_string):
    cleaned_text = re.sub(r'[^a-zA-Z]', ' ', input_string)
    language, confidence = langid.classify(cleaned_text)
    return language

df['bookLang'] = df["bookTitle"].apply(detect_lang)

df.head()

processed_csv_path = '/content/drive/MyDrive/CAPSTONE/processed_books_data.csv'
df.to_csv(processed_csv_path, index=False)

processed_json_path = '/content/drive/MyDrive/CAPSTONE/processed_books_data.json'
df.to_json(processed_json_path, orient='records', lines=True)

"""# Cosine Similarity

Cosine similarity is simply the similarity between many numbers so first we need to convert the data into numbers for that purpose we will use TF/IDF
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity,euclidean_distances

"""For TF/IDF we first need to gather all genres and language into a single string so lets do that"""

x=df.iloc[0]
x

result_string = " ".join(x.cleaned_bookGenres)
result_string=result_string+" "+x.bookLang
result_string

"""So now we will create a function to do this for all the dataset"""

def get_string(row):
    result_string = " ".join(row.cleaned_bookGenres)
    result_string=result_string+" "+row.bookLang
    return  result_string
df['string'] = df.apply(get_string,axis=1)

df.head()

tfidf=TfidfVectorizer(max_features=3000)

vector=tfidf.fit_transform(df['string'])
vector.shape

"""We need the list of all the Book Titles and there index in the dataset"""

book2idx = pd.Series(df.index, index=df['bookTitle'])
book2idx

"""Now we will calculate the Cosine Similarity of all the other books

Now we will sort the scores array and select the one with most similarity as we know argsort will sort in ascending order so we will add a - sign to the scores and get the top 5 values

So these are the books recommended for the users who love 'The Hunger Games'
So now we will make a function that take the title of book as input and give the recommendations.Here i want to add a functionality so that if the title entered by user is a little bit change or mis spelled then rather than giving Book not exist it sees if there is any Book name similar to the given Title
"""

def recommended_books_cosine(title):
    try:
        idx = book2idx[title]
    except KeyError:
        matches = process.extract(title, df['bookTitle'].tolist(), limit=1)
        if matches and matches[0][1] >= 80:
            similar_name = matches[0][0]
            return f"Did you mean '{similar_name}'?"
        return "Book does not exist"
    query = vector[idx]
    scores = cosine_similarity(query, vector)
    scores = scores.flatten()
    recommended_idx = (-scores).argsort()[1:6]
    return df['bookTitle'].iloc[recommended_idx].tolist()

"""# Jaccof Similarity"""

# Implementing Jaccard similarity based recommendation for genre
def find_recommendation_jaccard_genre(genre):
    genre = set([genre])
    temp = df.copy()
    temp['score'] = temp['cleaned_bookGenres'].apply(lambda x: len(x.intersection(genre)) / len(x.union(genre)))
    temp = temp.sort_values(by='score', ascending=False)
    top_5_rows = temp.iloc[:5, :]
    top_5_recommendation = top_5_rows['bookTitle'].tolist()
    return top_5_recommendation

# Implementing Jaccard similarity based recommendation
def find_recommendation_jaccard(name):
    if name in df['bookTitle'].tolist():
        inputset = df.loc[df['bookTitle'] == name, 'set'].iloc[0]
        temp = df[df['bookTitle'] != name]
        temp['score'] = temp.apply(lambda row: calculate_score(row, inputset), axis=1)
        temp = temp.sort_values(by='score', ascending=False)
        top_5_rows = temp.iloc[:5, :]
        top_5_recommendation = top_5_rows['bookTitle'].tolist()
        return top_5_recommendation
    matches = process.extract(name, df['bookTitle'].tolist(), limit=1)
    if matches and matches[0][1] >= 80:
        similar_name = matches[0][0]
        return f"Did you mean '{similar_name}'?"
    return f"'{name}' does not exist in the dataset."

# Implementing Jaccard Similarity function for cleaned genres and language
def convert_into_set(row):
    strlist = row['cleaned_bookGenres']
    strlist.add(row.bookLang)
    return strlist

df['set'] = df.apply(convert_into_set, axis=1)

def calculate_score(row, inputset):
    intersection = len(row['set'].intersection(inputset))
    union = len(row['set'].union(inputset))
    return intersection / union

"""# Algorithm"""

import joblib

# Save TF-IDF model
tfidf_model_path = '/content/drive/MyDrive/CAPSTONE/tfidf_model.pkl'
joblib.dump(tfidf, tfidf_model_path)

# Save TF-IDF vectors
tfidf_vectors_path = '/content/drive/MyDrive/CAPSTONE/tfidf_vectors.pkl'
joblib.dump(vector, tfidf_vectors_path)

# Save the dataframe and mappings using pandas
df.to_pickle('/content/drive/MyDrive/CAPSTONE/books_data.pkl')
book2idx.to_pickle('/content/drive/MyDrive/CAPSTONE/book2idx.pkl')

import joblib
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

# Load the dataframe and mappings
df = pd.read_pickle('/content/drive/MyDrive/CAPSTONE/books_data.pkl')
book2idx = pd.read_pickle('/content/drive/MyDrive/CAPSTONE/book2idx.pkl')

# Load TF-IDF model and vectors
tfidf_model_path = '/content/drive/MyDrive/CAPSTONE/tfidf_model.pkl'
tfidf_vectors_path = '/content/drive/MyDrive/CAPSTONE/tfidf_vectors.pkl'

tfidf = joblib.load(tfidf_model_path)
vector = joblib.load(tfidf_vectors_path)

# Verify the shape of the vector
print("TF-IDF vector shape:", vector.shape)
print("DataFrame shape:", df.shape)

# Adjusting indices if needed
if vector.shape[0] != df.shape[0]:
    print("Mismatch in indices. Adjusting...")
    df = df.iloc[:vector.shape[0]]

# Function for cosine similarity based recommendation
def recommended_books_cosine(title):
    try:
        idx = book2idx[title]
    except KeyError:
        matches = process.extract(title, df['bookTitle'].tolist(), limit=1)
        if matches and matches[0][1] >= 80:
            similar_name = matches[0][0]
            return f"Did you mean '{similar_name}'?"
        return "Book does not exist"
    if isinstance(idx, pd.Series):
        idx = idx.iloc[0]
    query = vector[idx]
    scores = cosine_similarity(query, vector)
    scores = scores.flatten()
    recommended_idx = (-scores).argsort()[1:6]
    return df['bookTitle'].iloc[recommended_idx].tolist()

# Function for Jaccard similarity based recommendation
def find_recommendation_jaccard(name):
    if name in df['bookTitle'].tolist():
        inputset = df.loc[df['bookTitle'] == name, 'set'].iloc[0]
        temp = df[df['bookTitle'] != name]
        temp['score'] = temp.apply(lambda row: calculate_score(row, inputset), axis=1)
        temp = temp.sort_values(by='score', ascending=False)
        top_5_rows = temp.iloc[:5, :]
        top_5_recommendation = top_5_rows['bookTitle'].tolist()
        return top_5_recommendation
    matches = process.extract(name, df['bookTitle'].tolist(), limit=1)
    if matches and matches[0][1] >= 80:
        similar_name = matches[0][0]
        return f"Did you mean '{similar_name}'?"
    return f"'{name}' does not exist in the dataset."

# Jaccard Similarity functions
def convert_into_set(row):
    strlist = row['cleaned_bookGenres']
    strlist.add(row.bookLang)
    return strlist

df['set'] = df.apply(convert_into_set, axis=1)

def calculate_score(row, inputset):
    intersection = len(row['set'].intersection(inputset))
    union = len(row['set'].union(inputset))
    return intersection / union

# Example usage
print(recommended_books_cosine("The Alchemist"))
print(find_recommendation_jaccard("The Alchemist"))

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input

# Load the data and models as before
import joblib
import pandas as pd

# Load the dataframe and mappings
df = pd.read_pickle('/content/drive/MyDrive/CAPSTONE/books_data.pkl')
book2idx = pd.read_pickle('/content/drive/MyDrive/CAPSTONE/book2idx.pkl')

# Load TF-IDF model and vectors
tfidf_model_path = '/content/drive/MyDrive/CAPSTONE/tfidf_model.pkl'
tfidf_vectors_path = '/content/drive/MyDrive/CAPSTONE/tfidf_vectors.pkl'

tfidf = joblib.load(tfidf_model_path)
vector = joblib.load(tfidf_vectors_path)

# Ensure indices match
if vector.shape[0] != df.shape[0]:
    df = df.iloc[:vector.shape[0]]

# Create a simple neural network model
model = Sequential()
model.add(Input(shape=(vector.shape[1],)))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='linear'))

model.compile(optimizer='adam', loss='mse')

# Save the model in H5 format
model_path = '/content/drive/MyDrive/CAPSTONE/tfidf_model.h5'
model.save(model_path)

print(f"Model saved to {model_path}")

import os
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input

# Ensure the directory exists
model_directory = '/content/drive/MyDrive/CAPSTONE'
if not os.path.exists(model_directory):
    print("Directory does not exist. Creating the directory.")
    os.makedirs(model_directory)

# Define the model path
model_path = os.path.join(model_directory, 'tfidf_model.h5')

# Create a simple neural network model
model = Sequential()
model.add(Input(shape=(vector.shape[1],)))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='linear'))

model.compile(optimizer='adam', loss='mse')

# Save the model
model.save(model_path)
print(f"Model saved to {model_path}")

# Verify the saved model file
if os.path.exists(model_path):
    print(f"Model file found at {model_path}")
else:
    print(f"Model file not found at {model_path}")

# Load the Keras model
try:
    loaded_model = tf.keras.models.load_model(model_path)
    print("Model loaded successfully.")
except OSError as e:
    print(f"Error loading model: {e}")

# Example of how to use the model (predict on a sample)
import numpy as np

# Assuming you want to predict for the first book's TF-IDF vector
sample_vector = vector[0].toarray()  # Convert to dense array if it's sparse
prediction = loaded_model.predict(sample_vector)
print("Prediction for the sample:", prediction)

# Convert sparse matrix to dense format
dense_vector = vector.toarray()

# Function to convert numpy types to native Python types
def convert_numpy_types(data):
    if isinstance(data, np.ndarray):
        return data.tolist()
    elif isinstance(data, np.integer):
        return int(data)
    elif isinstance(data, np.floating):
        return float(data)
    elif isinstance(data, dict):
        return {k: convert_numpy_types(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [convert_numpy_types(i) for i in data]
    return data

# Save dense vectors to JSON
tfidf_vectors_path = '/content/drive/MyDrive/CAPSTONE/tfidf_vectors.json'
with open(tfidf_vectors_path, 'w') as f:
    json.dump(convert_numpy_types(dense_vector), f)

# Save TF-IDF model components to JSON
tfidf_model_path = '/content/drive/MyDrive/CAPSTONE/tfidf_model.json'
tfidf_data = {
    "vocabulary_": convert_numpy_types(tfidf.vocabulary_),
    "idf_": convert_numpy_types(tfidf.idf_),
    "stop_words_": convert_numpy_types(list(tfidf.stop_words_)) if tfidf.stop_words_ is not None else []
}
with open(tfidf_model_path, 'w') as f:
    json.dump(tfidf_data, f)

# Save the dataframe and mappings using pandas
df.to_pickle('/content/drive/MyDrive/CAPSTONE/books_data.pkl')
book2idx.to_pickle('/content/drive/MyDrive/CAPSTONE/book2idx.pkl')

"""# Load and Use the Saved Models"""

# Load the dataframe and mappings
df = pd.read_pickle('/content/drive/MyDrive/CAPSTONE/books_data.pkl')
book2idx = pd.read_pickle('/content/drive/MyDrive/CAPSTONE/book2idx.pkl')

# Load dense vectors from JSON
tfidf_vectors_path = '/content/drive/MyDrive/CAPSTONE/tfidf_vectors.json'
with open(tfidf_vectors_path, 'r') as f:
    dense_vector = np.array(json.load(f))

# Convert dense vectors back to sparse matrix
from scipy.sparse import csr_matrix
vector = csr_matrix(dense_vector)

# Load TF-IDF model components from JSON
tfidf_model_path = '/content/drive/MyDrive/CAPSTONE/tfidf_model.json'
with open(tfidf_model_path, 'r') as f:
    tfidf_data = json.load(f)

# Recreate the TF-IDF vectorizer with the loaded data
tfidf = TfidfVectorizer()
tfidf.vocabulary_ = tfidf_data["vocabulary_"]
tfidf.idf_ = np.array(tfidf_data["idf_"])
tfidf.stop_words_ = set(tfidf_data["stop_words_"])

# Example of using the loaded models
print(recommended_books_cosine("The Hunger Games"))
print(find_recommendation_jaccard("fantasy"))

# Load the dataframe and mappings
df = pd.read_pickle('/content/drive/MyDrive/CAPSTONE/books_data.pkl')
book2idx = pd.read_pickle('/content/drive/MyDrive/CAPSTONE/book2idx.pkl')

# Load dense vectors from JSON
tfidf_vectors_path = '/content/drive/MyDrive/CAPSTONE/tfidf_vectors.json'
with open(tfidf_vectors_path, 'r') as f:
    dense_vector = np.array(json.load(f))

# Convert dense vectors back to sparse matrix
from scipy.sparse import csr_matrix
vector = csr_matrix(dense_vector)

# Load TF-IDF model components from JSON
tfidf_model_path = '/content/drive/MyDrive/CAPSTONE/tfidf_model.json'
with open(tfidf_model_path, 'r') as f:
    tfidf_data = json.load(f)

# Recreate the TF-IDF vectorizer with the loaded data
tfidf = TfidfVectorizer()
tfidf.vocabulary_ = tfidf_data["vocabulary_"]
tfidf.idf_ = np.array(tfidf_data["idf_"])
tfidf.stop_words_ = set(tfidf_data["stop_words_"])

# Example of using the loaded models
print(recommended_books_cosine("The Alchemist"))  # Example for title recommendation
print(find_recommendation_jaccard_genre("romance"))  # Example for genre recommendation

"""#####"""